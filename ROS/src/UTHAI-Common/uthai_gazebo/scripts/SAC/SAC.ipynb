{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hNA9hN6kvvnc"
   },
   "outputs": [],
   "source": [
    "#please run roscore on terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ESSYYOwkvvne"
   },
   "outputs": [],
   "source": [
    "#please run gazebo world on terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1IGNMZ5Evvne"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_218951/2467482076.py:47: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 10:04:36.493210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-05 10:04:37.677124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:37.682583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:37.682851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:37.978183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:37.978475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:37.978699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:37.978934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 2126 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import gc\n",
    "gc.enable()\n",
    "import rospy\n",
    "import roslib\n",
    "import rospy\n",
    "import rostopic\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "from std_srvs.srv import Empty\n",
    "from gazebo_msgs.srv import SetModelConfiguration\n",
    "from control_msgs.msg import JointControllerState\n",
    "from sensor_msgs.msg import JointState\n",
    "from gazebo_msgs.msg import LinkStates\n",
    "from gazebo_msgs.msg import ContactsState\n",
    "from geometry_msgs.msg import Pose\n",
    "from std_msgs.msg import Float64\n",
    "from std_msgs.msg import String\n",
    "from sensor_msgs.msg import Joy\n",
    "from gazebo_msgs.srv import DeleteModel\n",
    "from gazebo_msgs.srv import SpawnModel\n",
    "from controller_manager_msgs.srv import LoadController\n",
    "from controller_manager_msgs.srv import SwitchController\n",
    "\n",
    "import threading\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "gpu_available = tf.test.is_gpu_available()\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zlaa_UTXvvnf"
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'BipedalWalker-UTHAI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0yLopuYMvvng"
   },
   "outputs": [],
   "source": [
    "LAP = rospy.Publisher(\n",
    "        'uthai/l_ankle_pitch_position/command', Float64, queue_size=10)\n",
    "LAR = rospy.Publisher(\n",
    "    'uthai/l_ankle_roll_position/command', Float64, queue_size=10)\n",
    "LHP = rospy.Publisher(\n",
    "    'uthai/l_hip_pitch_position/command', Float64, queue_size=10)\n",
    "LHR = rospy.Publisher('uthai/l_hip_roll_position/command',\n",
    "                     Float64, queue_size=10)\n",
    "#LHY = rospy.Publisher('uthai/l_hip_yaw_position/command',\n",
    "#                      Float64, queue_size=10)\n",
    "LKP = rospy.Publisher(\n",
    "    'uthai/l_knee_pitch_position/command', Float64, queue_size=10)\n",
    "RAP = rospy.Publisher(\n",
    "    'uthai/r_ankle_pitch_position/command', Float64, queue_size=10)\n",
    "RAR = rospy.Publisher(\n",
    "    'uthai/r_ankle_roll_position/command', Float64, queue_size=10)\n",
    "RHP = rospy.Publisher(\n",
    "    'uthai/r_hip_pitch_position/command', Float64, queue_size=10)\n",
    "RHR = rospy.Publisher('uthai/r_hip_roll_position/command',\n",
    "                      Float64, queue_size=10)\n",
    "#RHY = rospy.Publisher('uthai/r_hip_yaw_position/command',\n",
    "#                      Float64, queue_size=10)\n",
    "RKP = rospy.Publisher(\n",
    "    'uthai/r_knee_pitch_position/command', Float64, queue_size=10)\n",
    "\n",
    "reset_simulation = rospy.ServiceProxy('/gazebo/reset_world', Empty)\n",
    "\n",
    "reset_joints = rospy.ServiceProxy('/gazebo/set_model_configuration', SetModelConfiguration)\n",
    "\n",
    "unpause = rospy.ServiceProxy('/gazebo/unpause_physics', Empty)\n",
    "\n",
    "pause = rospy.ServiceProxy('/gazebo/pause_physics', Empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "khFcTCjHvvng"
   },
   "outputs": [],
   "source": [
    "fall = 0\n",
    "rospy.init_node(\"q_set_control_node\")\n",
    "rate = rospy.Rate(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_H4lYv4zvvng"
   },
   "outputs": [],
   "source": [
    "class RobotState(object):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.outer_ring_inner_ring_theta = 0.0\n",
    "        self.mean = 0\n",
    "        self.last_mean =0\n",
    "        self.r_ankle =0.0\n",
    "        \n",
    "        self.l_ankle = 0.0\n",
    "        \n",
    "        self.waist_z = 0.0\n",
    "\n",
    "        \n",
    "        self.LAP_theta = 0.0\n",
    "        self.LAP_theta_dot = 0.0\n",
    "        \n",
    "        self.LAR_theta = 0.0\n",
    "        self.LAR_theta_dot = 0.0\n",
    "        \n",
    "        self.LHP_theta = 0.0\n",
    "        self.LHP_theta_dot = 0.0\n",
    "        \n",
    "        self.LHR_theta = 0.0\n",
    "        self.LHR_theta_dot = 0.0\n",
    "        #self.LHY_theta = 0.0\n",
    "        #self.LHY_theta_dot = 0.0\n",
    "        \n",
    "        self.LKP_theta = 0.0\n",
    "        self.LKP_theta_dot = 0.0\n",
    "        \n",
    "        self.RAP_theta = 0.0\n",
    "        self.RAP_theta_dot = 0.0\n",
    "        \n",
    "        self.RAR_theta = 0.0\n",
    "        self.RAR_theta_dot = 0.0\n",
    "        \n",
    "        self.RHP_theta = 0.0\n",
    "        self.RHP_theta_dot = 0.0\n",
    "        \n",
    "        self.RHR_theta = 0.0\n",
    "        self.RHR_theta_dot = 0.0\n",
    "        \n",
    "        #self.RHY_theta = 0.0\n",
    "        #self.RHY_theta_dot = 0.0\n",
    "        \n",
    "        self.RKP_theta = 0.0\n",
    "        self.RKP_theta_dot = 0.0\n",
    "        \n",
    " \n",
    "        #self.footr_contact = 0\n",
    "        #self.footl_contact = 0\n",
    "        #                                                      LAR\n",
    "        #                                 LHR\n",
    "        #RAR\n",
    "        #RHR\n",
    "        \n",
    "        self.robot_state = [self.LAP_theta, self.LAP_theta_dot,self.LAR_theta,self.LAR_theta_dot,\\\n",
    "        self.LHP_theta, self.LHP_theta_dot,self.LHR_theta,self.LHR_theta_dot,\\\n",
    "        self.LKP_theta,self.LKP_theta_dot,self.RAP_theta,self.RAP_theta_dot,\\\n",
    "        self.RHP_theta,self.RHP_theta_dot,self.RHR_theta,self.RHR_theta_dot,\\\n",
    "        self.RKP_theta,self.RKP_theta_dot,self.r_ankle , self.l_ankle,self.waist_z]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.robot_state_vel = [self.LAP_theta_dot, \\\n",
    "        self.LHP_theta_dot, \\\n",
    "        self.LKP_theta_dot,self.RAP_theta_dot,\\\n",
    "        self.RHP_theta_dot,\\\n",
    "        self.RKP_theta_dot]\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.latest_reward = 0.0\n",
    "        self.best_reward = -100000000000000.0\n",
    "        self.episode = 0\n",
    "        self.last_outer_ring_inner_ring_theta = 0.0\n",
    "        self.last_time = 0.0\n",
    "\n",
    "        self.fall = 0\n",
    "        self.done = False\n",
    "        self.count_of_1 = 0\n",
    "        self.avg_reward = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "d-FUA7ggvvnh"
   },
   "outputs": [],
   "source": [
    "class Publisher(threading.Thread):\n",
    "    def __init__(self,LAP,LAR ,LHP,LHR,LKP,RAP,RAR,RHP,RHR,RKP, rate):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.counter = 0\n",
    "        self.LAP = LAP\n",
    "        self.LAR = LAR\n",
    "        self.LHP = LHP\n",
    "        self.LHR = LHR\n",
    "        #self.LHY = LHY\n",
    "        self.LKP = LKP\n",
    "        self.RAP = RAP\n",
    "        self.RAR = RAR\n",
    "        self.RHP = RHP\n",
    "        self.RHR = RHR\n",
    "        #self.RHY = RHY\n",
    "        self.RKP = RKP\n",
    "        \n",
    "        self.rate = rate\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        publisher(self.LAP,self.LAR,self.LHP,self.LHR,self.LKP,self.RAP,self.RAR,self.RHP,\\\n",
    "        self.RHR,self.RKP,self.rate, self.counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3_LKr_xcvvnh"
   },
   "outputs": [],
   "source": [
    "robot_state = RobotState()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0chrd9zdvvni"
   },
   "outputs": [],
   "source": [
    "def set_robot_state():\n",
    "    robot_state.robot_state =  [robot_state.LAP_theta, robot_state.LAP_theta_dot,robot_state.LAR_theta,robot_state.LAR_theta_dot,\\\n",
    "        robot_state.LHP_theta, robot_state.LHP_theta_dot,robot_state.LHR_theta,robot_state.LHR_theta_dot,\\\n",
    "        robot_state.LKP_theta,robot_state.LKP_theta_dot,robot_state.RAP_theta,robot_state.RAP_theta_dot,\\\n",
    "        robot_state.RHP_theta,robot_state.RHP_theta_dot,robot_state.RHR_theta,robot_state.RHR_theta_dot,\\\n",
    "        robot_state.RKP_theta,robot_state.RKP_theta_dot,robot_state.r_ankle ,robot_state.l_ankle, robot_state.waist_z]\n",
    "    \"\"\"\n",
    "    robot_state.robot_state_vel= [robot_state.LAP_theta_dot, \\\n",
    "        robot_state.LHP_theta_dot, \\\n",
    "        robot_state.LKP_theta_dot,robot_state.RAP_theta_dot,\\\n",
    "        robot_state.RHP_theta_dot,\\\n",
    "        robot_state.RKP_theta_dot]\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L9jpXkJRvvni"
   },
   "outputs": [],
   "source": [
    "def reset_joint_hassan():\n",
    "    #LHY.publish(0)\n",
    "    #RHY.publish(0)\n",
    "    LAR.publish(0)\n",
    "    LHR.publish(0)\n",
    "    RAR.publish(0)\n",
    "    RHR.publish(0)\n",
    "\n",
    "    LHP.publish(0)\n",
    "    LKP.publish(0)\n",
    "    LAP.publish(0)\n",
    "    RHP.publish(0)\n",
    "    RKP.publish(0)\n",
    "    RAP.publish(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8hxeLGlCvvni"
   },
   "outputs": [],
   "source": [
    "def Reset() :\n",
    "     for _ in range(2):\n",
    "            reset()\n",
    "            \n",
    "     try:\n",
    "        pause()\n",
    "     except (rospy.ServiceException) as e:\n",
    "        print (\"rospause failed!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yavlZhCKvvni"
   },
   "outputs": [],
   "source": [
    "def reset():\n",
    "    # ['waist_thighR', 'waist_thighL', 'thighR_shankR', 'thighL_shankL', 'outer_ring_inner_ring', 'inner_ring_boom', 'boom_waist']\n",
    "\n",
    "    rospy.wait_for_service('gazebo/set_model_configuration')\n",
    "    \n",
    "    try:\n",
    "        reset_joint_hassan()\n",
    "        #delay_error()\n",
    "        robot_state.last_outer_ring_inner_ring_theta = 0.0\n",
    "        \n",
    "        \n",
    "    except (rospy.ServiceException) as e:\n",
    "        print (\"reset_joints failed!\")\n",
    "    \n",
    "    rospy.wait_for_service('/gazebo/pause_physics')\n",
    "\n",
    "    \n",
    "    rospy.wait_for_service('gazebo/reset_world')\n",
    "    try:\n",
    "        reset_simulation()\n",
    "        \n",
    "    except(rospy.ServiceException) as e :\n",
    "        print (\"reset_world failed!\")\n",
    "        \n",
    "    #time.sleep(1)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pause()\n",
    "        print(\"-------------paused_simulation-----------\")\n",
    "    except (rospy.ServiceException) as e:\n",
    "        print (\"rospause failed!'\")\n",
    "    \"\"\"\n",
    "    set_robot_state()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_reset():        \n",
    "        rospy.wait_for_service('/gazebo/delete_model')\n",
    "        try:\n",
    "                DeleteModel()\n",
    "\n",
    "        except (rospy.ServiceException) as e:\n",
    "                print (\"delete model  failed!\")\n",
    "        rospy.wait_for_service('/gazebo/spawn_urdf_model')\n",
    "        try:\n",
    "                SpawnModel()\n",
    "\n",
    "        except (rospy.ServiceException) as e:\n",
    "\n",
    "               print (\"spawn_urdf_model failed!\")\n",
    "        try:\n",
    "           pause()\n",
    "        except (rospy.ServiceException) as e:\n",
    "           print (\"pause failed!'\")\n",
    "        rospy.wait_for_service('gazebo/set_model_configuration')\n",
    "\n",
    "        try:\n",
    "                reset_joint_hassan()\n",
    "                robot_state.last_outer_ring_inner_ring_theta = 0.0\n",
    "                robot_state.last_mean = 0.0\n",
    "        except (rospy.ServiceException) as e:\n",
    "                print (\"reset_joints failed!\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "           unpause()\n",
    "        except (rospy.ServiceException) as e:\n",
    "           print (\"unpause failed!'\")\n",
    "        \n",
    "        rospy.wait_for_service(\"/uthai/controller_manager/load_controller\")\n",
    "        \n",
    "        error_compensator()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        try:\n",
    "           LoadController()\n",
    "        except (rospy.ServiceException) as e:\n",
    "           print (\"LoadController failed!'\")\n",
    "        rospy.wait_for_service(\"/uthai/controller_manager/switch_controller\")\n",
    "        try:\n",
    "           SwitchController()\n",
    "        except (rospy.ServiceException) as e:\n",
    "           print (\"SwitchController failed!\")\n",
    "        rospy.wait_for_service('gazebo/reset_world')\n",
    "        try:\n",
    "            reset_simulation()\n",
    "        \n",
    "        except(rospy.ServiceException) as e :\n",
    "            print (\"reset_world failed!\")\n",
    "        reset_joint_hassan()\n",
    "        while True :\n",
    "             if robot_state.waist_z > 0.55 and robot_state.waist_z < 0.9 :\n",
    "                    break\n",
    "        #error_compensator()\n",
    "        #time.sleep(1)\n",
    "        rospy.wait_for_service('/gazebo/pause_physics')\n",
    "        try:\n",
    "           pause()\n",
    "        except (rospy.ServiceException) as e:\n",
    "           print (\"rospause failed!'\")\n",
    "        set_robot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    # ['waist_thighR', 'waist_thighL', 'thighR_shankR', 'thighL_shankL', 'outer_ring_inner_ring', 'inner_ring_boom', 'boom_waist']\n",
    "    rospy.wait_for_service('gazebo/reset_world')\n",
    "    try:\n",
    "        reset_simulation()\n",
    "    except(rospy.ServiceException) as e:\n",
    "        print (\"reset_world failed!\")\n",
    "\n",
    "\n",
    "    rospy.wait_for_service('gazebo/set_model_configuration')\n",
    "\n",
    "    try:\n",
    "        reset_joints(\"uthai\", \"robot_description\", [\"l_ankle_pitch_joint\",\"l_ankle_roll_joint\",\"l_hip_pitch_joint\",\"l_knee_pitch_joint\",\"l_hip_roll_joint\",\"r_ankle_pitch_joint\" ,\"r_ankle_roll_joint\",\"r_hip_pitch_joint\",\"r_hip_roll_joint\",\"r_knee_pitch_joint\"], \n",
    "                     [0.0,0.0,0.0, 0.0, 0.0, 0.0,0.0,0.0,0.0,0.0])\n",
    "        #reset_joint_hassan()\n",
    "        #error_compensator()\n",
    "        robot_state.last_outer_ring_inner_ring_theta = 0.0\n",
    "    except (rospy.ServiceException) as e:\n",
    "        print (\"reset_joints failed!\")\n",
    "\n",
    "    rospy.wait_for_service('/gazebo/pause_physics')\n",
    "    try:\n",
    "        pause()\n",
    "    except (rospy.ServiceException) as e:\n",
    "        print (\"rospause failed!\")\n",
    "\n",
    "    set_robot_state()\n",
    "\n",
    "    # print \"called reset()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "l62X6hmlvvnp"
   },
   "outputs": [],
   "source": [
    "def error_compensator() :\n",
    "    \n",
    "    st= time.time()\n",
    "    global break_loop\n",
    "    while True : \n",
    "         a = tf.constant([robot_state.LAP_theta ,robot_state.LHR_theta,robot_state.LHP_theta ,\n",
    "robot_state.LKP_theta,robot_state.RAR_theta, robot_state.RAP_theta,\n",
    "robot_state.RHP_theta ,robot_state.RKP_theta ,robot_state.LAR_theta,robot_state.RHR_theta])\n",
    "         b = tf.constant([robot_state.LAP_theta_dot ,robot_state.LHR_theta_dot,robot_state.LHP_theta_dot ,\n",
    "robot_state.LKP_theta_dot,robot_state.RAR_theta_dot, robot_state.RAP_theta_dot,\n",
    "robot_state.RHP_theta_dot ,robot_state.RKP_theta_dot ,robot_state.LAR_theta_dot,robot_state.RHR_theta_dot])\n",
    "         error_norm_b = tf.norm(b,ord=2)#norm1\n",
    "         error_norm_a = tf.norm(a,ord=2)#norm1\n",
    "         #print(tf.norm(b,ord=2))\n",
    "         if error_norm_a < 0.1 or abs(robot_state.outer_ring_inner_ring_theta)>9:\n",
    "                break\n",
    "         if abs(st-time.time()) > 6 :\n",
    "                break_loop = True\n",
    "                print(\"-----------tool_keshid_oomadam_biroon--------\")\n",
    "                #break\n",
    "                while True :\n",
    "                    a=1\n",
    "                    pass\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "D9aH7y3zvvnj"
   },
   "outputs": [],
   "source": [
    "def take_action(action):\n",
    "    rospy.wait_for_service('/gazebo/unpause_physics')\n",
    "\n",
    "    try:\n",
    "        unpause()\n",
    "    except (rospy.ServiceException) as e:\n",
    "        print (\"/gazebo/pause_physics service call failed\")\n",
    "    last_time = time.time()\n",
    "    LAP.publish(action[0])\n",
    "    LAR.publish(action[1])\n",
    "    LHP.publish(action[2])\n",
    "    LHR.publish(action[3])\n",
    "    #LHY.publish(action[2])\n",
    "    LKP.publish(action[4])\n",
    "    RAP.publish(action[5])\n",
    "    RAR.publish(action[6])\n",
    "    RHP.publish(action[7])\n",
    "    RHR.publish(action[8])\n",
    "    #RHY.publish(action[6])\n",
    "    RKP.publish(action[9])\n",
    "\n",
    "\n",
    "    reward = -0.1  # when it used to run, used to be -0.1\n",
    "    #reward = 1\n",
    "    #sky= (robot_state.r_ankle < 0.2 or  robot_state.l_ankle <0.2)\n",
    "    #print(robot_state.waist_z)\n",
    "    #print(\"time : \" ,current_time- last_time)\n",
    "    \n",
    "    if robot_state.waist_z > 0.5 and robot_state.waist_z < 0.9  :#0.36\n",
    "            \n",
    "            #print(\"ekhtelaf:\",robot_state.outer_ring_inner_ring_theta - robot_state.last_outer_ring_inner_ring_theta)\n",
    "            #print(robot_state.mean - robot_state.last_mean)\n",
    "            if  (robot_state.outer_ring_inner_ring_theta - robot_state.last_outer_ring_inner_ring_theta) >= 0.0: #-0.001forward motion\n",
    "                         #if (robot_state.mean - robot_state.last_mean)>0.01 :\n",
    "            \n",
    "                         #print(\"delta-time :\" , delta_time)\n",
    "                         #dt= time.time() - last_time\n",
    "            \n",
    "                         reward += ((robot_state.outer_ring_inner_ring_theta - robot_state.last_outer_ring_inner_ring_theta)*100)\n",
    "                         #print(\"reward\" ,(robot_state.outer_ring_inner_ring_theta - robot_state.last_outer_ring_inner_ring_theta)/dt)\n",
    "                         #reward += ((robot_state.mean - robot_state.last_mean)*100)\n",
    "            \n",
    "    \n",
    "    else :\n",
    "        reward += -100\n",
    "        #print(\"i am in if 1\")\n",
    "        robot_state.done = True\n",
    "        robot_state.fall = 0\n",
    "        \n",
    "    #robot_state.last_time = current_time\n",
    "    robot_state.last_outer_ring_inner_ring_theta = robot_state.outer_ring_inner_ring_theta\n",
    "    #robot_state.last_mean = robot_state.mean\n",
    "    \n",
    "    if robot_state.outer_ring_inner_ring_theta > 6.0:\n",
    "        reward += 100\n",
    "        #print('i am in if 2')\n",
    "        robot_state.done = True\n",
    "        robot_state.fall = 1\n",
    "        #break_loop = True\n",
    "        print(\"-----------tool_keshid_oomadam_biroon--------\")\n",
    "        print (\"REACHED TO THE END!\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #\"outer_ring_inner_ring_theta :\",robot_state.outer_ring_inner_ring_theta,\n",
    "    #      \"last_outer_ring_inner_ring_theta:\",robot_state.last_outer_ring_inner_ring_theta,\n",
    "    #     \"ekhtelaf :\",robot_state.outer_ring_inner_ring_theta - robot_state.last_outer_ring_inner_ring_theta)\n",
    "    rate.sleep()\n",
    "    return reward, robot_state.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "fYh50AQzvvnj"
   },
   "outputs": [],
   "source": [
    "def callbackJointStates(data):\n",
    "\n",
    "    robot_state.data = data\n",
    "    \n",
    "\n",
    "    if len(data.velocity)!=0:\n",
    "        \n",
    "        robot_state.LAP_theta_dot = np.clip(data.velocity[0],-10,10)\n",
    "        \n",
    "        robot_state.LAR_theta_dot = np.clip(data.velocity[1],-10,10)\n",
    "        \n",
    "        robot_state.LHP_theta_dot = np.clip(data.velocity[2],-10,10)\n",
    "        \n",
    "        robot_state.LHR_theta_dot = np.clip(data.velocity[3],-10,10)\n",
    "        \n",
    "        #robot_state.LHY_theta_dot = data.velocity[2]\n",
    "        \n",
    "        robot_state.LKP_theta_dot = np.clip(data.velocity[4],-10,10)\n",
    "        \n",
    "        robot_state.RAP_theta_dot = np.clip(data.velocity[5],-10,10)\n",
    "        \n",
    "        robot_state.RAR_theta_dot = np.clip(data.velocity[6],-10,10)\n",
    "        \n",
    "        robot_state.RHP_theta_dot = np.clip(data.velocity[7],-10,10)\n",
    "        \n",
    "        robot_state.RHR_theta_dot = np.clip(data.velocity[8],-10,10)\n",
    "        \n",
    "        #robot_state.RHY_theta_dot = data.velocity[6]\n",
    "        \n",
    "        robot_state.RKP_theta_dot = np.clip(data.velocity[9],-10,10)\n",
    "    \n",
    "\n",
    "\n",
    "        robot_state.LAP_theta = data.position[0]\n",
    "        robot_state.LAR_theta = data.position[1]\n",
    "        robot_state.LHP_theta = data.position[2]\n",
    "        robot_state.LHR_theta = data.position[3]\n",
    "        #robot_state.LHY_theta = data.position[2]\n",
    "        robot_state.LKP_theta = data.position[4]\n",
    "        robot_state.RAP_theta = data.position[5]\n",
    "        robot_state.RAR_theta = data.position[6]\n",
    "        robot_state.RHP_theta = data.position[7]\n",
    "        robot_state.RHR_theta = data.position[8]\n",
    "        #robot_state.RHY_theta = data.position[6]\n",
    "        robot_state.RKP_theta = data.position[9]\n",
    "\n",
    "        \n",
    "\n",
    "    else:\n",
    "        robot_state.LAP_theta_dot = 0\n",
    "        \n",
    "        robot_state.LAR_theta_dot = 0\n",
    "        \n",
    "        robot_state.LHP_theta_dot = 0\n",
    "        \n",
    "        robot_state.LHR_theta_dot = 0\n",
    "        \n",
    "        #robot_state.LHY_theta_dot = 0\n",
    "        \n",
    "        robot_state.LKP_theta_dot = 0\n",
    "        \n",
    "        robot_state.RAP_theta_dot = 0\n",
    "        \n",
    "        robot_state.RAR_theta_dot = 0\n",
    "        \n",
    "        robot_state.RHP_theta_dot = 0\n",
    "        \n",
    "        robot_state.RHR_theta_dot = 0\n",
    "        \n",
    "        #robot_state.RHY_theta_dot = 0\n",
    "        \n",
    "        robot_state.RKP_theta_dot = 0\n",
    "\n",
    "        robot_state.LAP_theta = 0\n",
    "        robot_state.LAR_theta = 0\n",
    "        robot_state.LHP_theta = 0\n",
    "        robot_state.LHR_theta = 0\n",
    "        #robot_state.LHY_theta = 0\n",
    "        robot_state.LKP_theta = 0\n",
    "        robot_state.RAP_theta = 0\n",
    "        robot_state.RAR_theta = 0\n",
    "        robot_state.RHP_theta = 0\n",
    "        robot_state.RHR_theta = 0\n",
    "        #robot_state.RHY_theta = 0\n",
    "        robot_state.RKP_theta = 0\n",
    "\n",
    "    set_robot_state()\n",
    "    #rate.sleep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CAAcVkUsvvnj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def callbackSub(data):\n",
    "    set_robot_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BC0u_m_wvvnk"
   },
   "outputs": [],
   "source": [
    "def callbackContactShankR(data):\n",
    "    if not data.states:\n",
    "        robot_state.footr_contact = 0\n",
    "    else:\n",
    "        robot_state.footr_contact = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U63W9grrvvnk"
   },
   "outputs": [],
   "source": [
    "def callbackContactShankL(data):\n",
    "    if not data.states:\n",
    "        robot_state.footl_contact = 0\n",
    "    else:\n",
    "        robot_state.footl_contact = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jrMEzNYivvnk"
   },
   "outputs": [],
   "source": [
    "def Posecallback(data):\n",
    "    try:\n",
    "      ind = data.name.index(\"uthai::base_link\")\n",
    "      ind2_r = data.name.index(\"uthai::r_ankle_pitch_link\")\n",
    "      ind2_l= data.name.index(\"uthai::l_ankle_pitch_link\")\n",
    "\n",
    "      link_pose = data.pose[ind].position\n",
    "        \n",
    "      link_pose_r = data.pose[ind2_r].position\n",
    "      link_pose_l = data.pose[ind2_l].position\n",
    "\n",
    "      robot_state.outer_ring_inner_ring_theta = link_pose.x\n",
    "      robot_state.waist_z = link_pose.z\n",
    "\n",
    "      robot_state.r_ankle=link_pose_r.z\n",
    "      robot_state.l_ankle=link_pose_l.z\n",
    "      #robot_state.mean = (link_pose_r.x+link_pose_l.x)/2\n",
    "\n",
    "    except ValueError:\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Pn1qR28xvvnk"
   },
   "outputs": [],
   "source": [
    "def listener():\n",
    "    print (\"listener\")\n",
    "    #LAR.publish(0)\n",
    "    #LHR.publish(0)\n",
    "    #RAR.publish(0)\n",
    "    #RHR.publish(0)\n",
    "\n",
    "    rospy.Subscriber(\"/uthai/joint_states\", JointState, callbackJointStates)\n",
    "    \n",
    "    rospy.Subscriber(\"/gazebo/link_states\", LinkStates, Posecallback)\n",
    "    \n",
    "    \n",
    "    #rospy.Subscriber(\"/l_bumper_topic\",ContactsState,callback_contact_l)\n",
    "    #rospy.Subscriber(\"/r_bumper_topic\" ,ContactsState,callback_contact_r)\n",
    "    \n",
    "    #rospy.Subscriber(\"/footR_contact_sensor_state\", ContactsState, callbackContactShankR)\n",
    "    #rospy.Subscriber(\"/footL_contact_sensor_state\", ContactsState, callbackContactShankL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "791Sximhvvnk"
   },
   "outputs": [],
   "source": [
    "def callback_contact_l(data) :\n",
    "    if not data.states:\n",
    "        print(\"in not conditon\")\n",
    "        print(\"list :\" ,data.states[0])\n",
    "    else:\n",
    "        print(\"in true condtion\")\n",
    "        print(\"list:\"  ,data.states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_contact_r(data) :\n",
    "    \n",
    "    if not data.states:\n",
    "        print(\"in not conditon\")\n",
    "    else:\n",
    "        print(\"in true condtion\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "b4TvnA6i2mxH"
   },
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "\n",
    "    def __init__(self,action_dim,hidden_dim,action_high ,action_low):\n",
    "      \n",
    "        super(Actor, self).__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.dense1_layer = layers.Dense(hidden_dim, activation=tf.nn.relu,kernel_initializer = tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.dense2_layer = layers.Dense(hidden_dim, activation=tf.nn.relu,kernel_initializer = tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.mean_layer = layers.Dense(self.action_dim,kernel_initializer = tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.stdev_layer = layers.Dense(self.action_dim,kernel_initializer= tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "\n",
    "\n",
    "        self.action_scale = tf.constant(\n",
    "            ((action_high - action_low) / 2.),dtype = tf.float64)\n",
    "        self.action_bias = tf.constant(\n",
    "            ((action_high + action_low) / 2.),dtype = tf.float64)\n",
    "\n",
    "    def call(self, state):\n",
    "\n",
    "        a1 = self.dense1_layer(state)\n",
    "        a2 = self.dense2_layer(a1)\n",
    "\n",
    "\n",
    "        mean = self.mean_layer(a2)\n",
    "\n",
    "\n",
    "        log_std = self.stdev_layer(a2)\n",
    "        log_std= tf.clip_by_value(log_std, clip_value_min=LOG_SIG_MIN, clip_value_max=LOG_SIG_MAX)\n",
    "\n",
    "        std = tf.exp(log_std)\n",
    "\n",
    "        normal = tfp.distributions.Normal(mean, std)\n",
    "\n",
    "        x_t = normal.sample()\n",
    "\n",
    "        y_t = tf.tanh(x_t)\n",
    "\n",
    "\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        \n",
    "\n",
    "\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "\n",
    "        log_prob -= tf.math.log(self.action_scale*(1 -tf.pow(y_t, 2)) + epsilon)\n",
    "\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=1,keepdims=True)\n",
    "\n",
    "        mean = tf.tanh(mean) * self.action_scale + self.action_bias\n",
    "\n",
    "        return action, log_prob  , mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "29w8Bu4-HeZc"
   },
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "\n",
    "    def __init__(self,hidden_dim):\n",
    "      \n",
    "        super(Critic,self).__init__()\n",
    "\n",
    "        self.dense1_layer = layers.Dense(hidden_dim, activation=tf.nn.relu,kernel_initializer = tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.dense2_layer = layers.Dense(hidden_dim, activation=tf.nn.relu,kernel_initializer = tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.output1_layer = layers.Dense(1)\n",
    "\n",
    "        self.dense3_layer = layers.Dense(hidden_dim, activation=tf.nn.relu,kernel_initializer = tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.dense4_layer = layers.Dense(hidden_dim, activation=tf.nn.relu,kernel_initializer = tf.keras.initializers.GlorotUniform(),\n",
    "                          bias_initializer=tf.keras.initializers.Zeros())\n",
    "        self.output2_layer = layers.Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        state_action = tf.concat([state, action], axis=1)\n",
    "        a1 = self.dense1_layer(state_action)\n",
    "        a1= self.dense2_layer(a1)\n",
    "        q1 = self.output1_layer(a1)\n",
    "\n",
    "        a2 = self.dense3_layer(state_action)\n",
    "        a2 = self.dense4_layer(a2)\n",
    "        q2 = self.output2_layer(a2)\n",
    "        return q1,q2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DrzwpHBEDsVs"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, seed):\n",
    "        random.seed(seed)\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def save_buffer(self, env_name, suffix=\"\", save_path=None):\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = \"checkpoints/sac_buffer_{}_{}\".format(env_name, suffix)\n",
    "        print('Saving buffer to {}'.format(save_path))\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self.buffer, f)\n",
    "\n",
    "    def load_buffer(self, save_path):\n",
    "        print('Loading buffer from {}'.format(save_path))\n",
    "\n",
    "        with open(save_path, \"rb\") as f:\n",
    "            self.buffer = pickle.load(f)\n",
    "            self.position = len(self.buffer) % self.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftActorCritic:\n",
    "\n",
    "    def __init__(self,action_dim,action_high,action_low):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha#\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "\n",
    "        self.critic = Critic(hidden_size)#\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "\n",
    "        self.critic_target = Critic(hidden_size)#\n",
    "\n",
    "        self.critic_target.set_weights(self.critic.get_weights()) \n",
    "\n",
    "        hard_update(self.critic.variables,self.critic_target.variables)\n",
    "\n",
    "        if self.automatic_entropy_tuning is True:\n",
    "                self.target_entropy = -tf.constant(action_dim, dtype=tf.float64)#\n",
    "                self.log_alpha = tf.Variable(0.0, dtype=tf.float64)#\n",
    "                self.alpha_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "        self.policy = Actor(action_dim,hidden_size,action_high,action_low)#\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "\n",
    "\n",
    "    def sample_action(self, current_state,evaluate=False):\n",
    "        current_state_ = np.array(current_state, ndmin=2)\n",
    "        if evaluate is False:\n",
    "\n",
    "            action, _,_ = self.policy(current_state_)\n",
    "        else :\n",
    "            _, _,action = self.policy(current_state_)\n",
    "                        \n",
    "\n",
    "        return action[0]\n",
    "\n",
    "   \n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "\n",
    "              state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size=batch_size)\n",
    "\n",
    "              state_batch = tf.constant(state_batch,dtype =tf.float64)\n",
    "              next_state_batch = tf.constant(next_state_batch,dtype =tf.float64)\n",
    "              action_batch = tf.constant(action_batch,dtype =tf.float64)\n",
    "              reward_batch = tf.expand_dims(tf.constant(reward_batch,dtype =tf.float64), axis=1)\n",
    "              mask_batch = tf.expand_dims(tf.constant(mask_batch,dtype =tf.float64),axis=1)\n",
    "              with tf.GradientTape() as tape1:\n",
    "                  # Get Q value estimates, action used here is from the replay buffer\n",
    "\n",
    "                  next_state_action, next_state_log_pi,_ = self.policy(next_state_batch)\n",
    "\n",
    "                  # Get Q value estimates from target Q network\n",
    "                  qf1_next_target,qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "\n",
    "                  # Apply the clipped double Q trick\n",
    "                  # Get the minimum Q value of the 2 target networks\n",
    "                  min_qf_next_target = tf.minimum(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "\n",
    "                  #print(reward_batch.shape , mask_batch.shape , min_qf_next_target.shape)\n",
    "                  next_q_value = reward_batch + mask_batch * self.gamma * min_qf_next_target\n",
    "                  #print(next_q_value.shape)\n",
    "\n",
    "\n",
    "                 \n",
    "                  qf1,qf2 = self.critic(state_batch, action_batch)\n",
    "\n",
    "                  # Sample actions from the policy for next states\n",
    "\n",
    "\n",
    "\n",
    "                  # Add the entropy term to get soft Q target\n",
    "\n",
    "                  critic1_loss = tf.reduce_mean((qf1 - next_q_value)**2)\n",
    "\n",
    "             \n",
    "                  critic2_loss = tf.reduce_mean((qf2 - next_q_value)**2)\n",
    "\n",
    "                  critic_loss = critic1_loss + critic2_loss\n",
    "                  #print(next_q_value.shape , qf1.shape)\n",
    "                  #print(\"---------------------------------------------------\")\n",
    "\n",
    "              #print(\"critic_loss\",critic_loss)\n",
    "              grads1 = tape1.gradient(critic_loss, self.critic.trainable_variables)\n",
    "              self.critic_optimizer.apply_gradients(zip(grads1,\n",
    "                                                          self.critic.trainable_variables))\n",
    "\n",
    "              with tf.GradientTape() as tape3:\n",
    "                  # Sample actions from the policy for current states\n",
    "                  pi, log_pi,_ = self.policy(state_batch)\n",
    "\n",
    "\n",
    "                  # Get Q value estimates from target Q network\n",
    "                  qf1_pi , qf2_pi = self.critic(state_batch, pi)\n",
    "\n",
    "                  # Apply the clipped double Q trick\n",
    "                  # Get the minimum Q value of the 2 target networks\n",
    "                  min_qf_pi = tf.minimum(qf1_pi, qf2_pi)\n",
    "\n",
    "\n",
    "                  soft_q = min_qf_pi - self.alpha * log_pi\n",
    "\n",
    "\n",
    "                  actor_loss = -tf.reduce_mean(soft_q)\n",
    "                  #print(\"actor_loss\",actor_loss)\n",
    "\n",
    "              variables = self.policy.trainable_variables\n",
    "              grads3 = tape3.gradient(actor_loss, variables)\n",
    "              self.actor_optimizer.apply_gradients(zip(grads3, variables))\n",
    "\n",
    "              with tf.GradientTape() as tape4:\n",
    "                  # Sample actions from the policy for current states\n",
    "                  pi, log_pi,_ = self.policy(state_batch)\n",
    "                  alpha_loss = tf.reduce_mean(- self.log_alpha*(log_pi + self.target_entropy))\n",
    "                  #print(\"alpha_loss\",alpha_loss)\n",
    "\n",
    "              variables = [self.log_alpha]\n",
    "              #print([var.name for var in tape4.watched_variables()]\n",
    "              grads = tape4.gradient(alpha_loss, variables)\n",
    "\n",
    "\n",
    "              self.alpha_optimizer.apply_gradients(zip(grads, variables))\n",
    "              self.alpha= tf.exp(self.log_alpha)\n",
    "              if updates % self.target_update_interval == 0:\n",
    "\n",
    "        \n",
    "                   soft_update(self.critic.variables,self.critic_target.variables,self.tau)\n",
    "              return critic_loss , actor_loss , alpha_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(source_vars, target_vars, tau: float) -> None:\n",
    "\n",
    "    \n",
    "    if len(source_vars) != len(target_vars):\n",
    "        raise ValueError(\"source_vars and target_vars must have the same length.\")\n",
    "    for source, target in zip(source_vars, target_vars):\n",
    "        target.assign((1.0 - tau) * target + tau * source)\n",
    "\n",
    "\n",
    "def hard_update(source_vars, target_vars) -> None:\n",
    "\n",
    "    # Tau of 1, so get everything from source and keep nothing from target\n",
    "    soft_update(source_vars, target_vars, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDW_rx-xQP2h",
    "outputId": "d8a7f0dd-88f5-4a89-c9f2-7d5d3ebd3ee9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-05 10:04:47.300557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.300913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.301143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.301727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.302033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.302253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.302507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.302717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-05 10:04:47.302927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2126 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "eval =True\n",
    "\n",
    "gamma =0.99\n",
    "tau =0.005\n",
    "lr = 0.0003\n",
    "alpha=0.05\n",
    "automatic_entropy_tuning=True\n",
    "seed=123456\n",
    "batch_size=256\n",
    "num_steps=10000001\n",
    "hidden_size=256\n",
    "\n",
    "updates_per_step=1\n",
    "start_steps=10000\n",
    "\n",
    "target_update_interval=1\n",
    "replay_size=10000000\n",
    "\n",
    "\n",
    "np.random.seed(seed)\n",
    "#                             LAP,      LAR   ,LHP,  LHR,   LHY,LKP,  RAP,  RAR,  RHP,   RHR,  RHY,RKP\n",
    "new_action_min = tf.constant([ -0.349 ,-0.5 , -1.396 , -0.5,    0.0,-0.349, -0.5,-1.396, -0.5,    0],dtype =tf.float64)\n",
    "new_action_max = tf.constant([1.047   ,0.5  ,  1.396 , 0.5 ,    1.4, 1.047,  0.5,1.396,  0.5,    1.4],dtype =tf.float64)\n",
    "\n",
    "\n",
    "agent = SoftActorCritic(new_action_min.shape[0],new_action_max,new_action_min)\n",
    "\n",
    "memory = ReplayMemory(replay_size,seed)\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "uniform_sampling= tfp.distributions.Uniform(\n",
    "    low=new_action_min, \n",
    "    high=new_action_max, validate_args=False, allow_nan_stats=True,\n",
    "    name='Uniform'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#critic_loss_ = []\n",
    "#actor_loss_ = []\n",
    "#alpha_loss_ = []\n",
    "avg_reward = []\n",
    "av_reward =[]\n",
    "av_rewards=[]\n",
    "i_episode = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publisher(LAP,LAR,LHP,LHR,LKP,RAP,RAR,RHP,\\\n",
    "        RHR,RKP,rate,counter):\n",
    "\n",
    "        global i_episode\n",
    "\n",
    "        global total_numsteps\n",
    "        global updates \n",
    "        break_loop = False\n",
    "        global av_reward\n",
    "        global av_rewards\n",
    "        global time1\n",
    "        while not rospy.is_shutdown() :\n",
    "            \n",
    "\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            robot_state.done=False\n",
    "            \"\"\"\n",
    "            try:\n",
    "                    unpause()\n",
    "            except (rospy.ServiceException) as e:\n",
    "                    print (\"unpause failed!'\")\n",
    "            \"\"\"\n",
    "            time.sleep(0.5)\n",
    "            best_reset()\n",
    "            if break_loop == True:\n",
    "                    print(\"barnameh be payan resid\")\n",
    "                    break\n",
    "            state =robot_state.robot_state\n",
    "\n",
    "            i_episode= i_episode +1\n",
    "            while not done:\n",
    "\n",
    "                if start_steps > total_numsteps:\n",
    "                    action = uniform_sampling.sample()  # Sample random action\n",
    "                else:\n",
    "                    action = agent.sample_action(state)  # Sample action from policy\n",
    "                    \n",
    "                reward,done = take_action(action)\n",
    "                \"\"\"\n",
    "                try:\n",
    "                   pause()\n",
    "                except (rospy.ServiceException) as e:\n",
    "                   print (\"pause failed!'\")\n",
    "                \"\"\"\n",
    "                \n",
    "                if len(memory) > batch_size:\n",
    "                    # Number of updates per step in environment\n",
    "                    for i in range(updates_per_step):\n",
    "                        # Update parameters of all the networks\n",
    "                        critic_loss , actor_loss , alpha_loss= agent.update_parameters(memory, batch_size, updates)\n",
    "                        #critic_loss_.append(critic_loss)\n",
    "                        #actor_loss_.append(actor_loss)\n",
    "                        #alpha_loss_.append(alpha_loss)      \n",
    "                        updates += 1\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                next_state = robot_state.robot_state\n",
    "\n",
    "                episode_steps += 1\n",
    "                total_numsteps += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "                mask = float(not done)\n",
    "                #mask = 1 if episode_steps==1000 else float(not done)\n",
    "                if episode_steps==1  and done==True :\n",
    "                    pass\n",
    "                \n",
    "                else :\n",
    "\n",
    "                    memory.push(state, action, reward, next_state, mask)\n",
    "                    robot_state.fall=0\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            if total_numsteps > num_steps:\n",
    "                print(\"Start\")\n",
    "                break\n",
    "            #avg_reward.append(episode_reward/10)\n",
    "            print(\"Episode: {}, total numsteps: {}, episode steps: {}, reward: {} \".format(i_episode, total_numsteps, episode_steps, round(episode_reward, 2)))\n",
    "            av_reward.append(round(episode_reward, 2))\n",
    "\n",
    "            if i_episode % 100 == 0 and eval is True:\n",
    "                print(\"avereage of reward :\" ,sum(av_reward)/100)\n",
    "                av_rewards.append(sum(av_reward)/100)\n",
    "                av_reward=[]\n",
    "                agent.policy.save_weights(str(i_episode))\n",
    "                time.sleep(10)\n",
    "                \"\"\"\n",
    "                avg_reward = 0.\n",
    "                episodes = 5\n",
    "                for _  in range(episodes):\n",
    "                    best_reset()\n",
    "                    state =robot_state.robot_state\n",
    "                    episode_reward = 0\n",
    "                    done = False\n",
    "                    robot_state.done=False\n",
    "                    while not done:\n",
    "                        action = agent.sample_action(state, evaluate=True)\n",
    "                        \n",
    "                        reward,done = take_action(action)\n",
    "                        next_state = robot_state.robot_state\n",
    "                        episode_reward += reward\n",
    "\n",
    "\n",
    "                        state = next_state               \n",
    "\n",
    "\n",
    "                    print(\"reward: {} \".format(round(episode_reward, 2)))\n",
    "                    print(\"----------------------------------------\")\n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LkkG5Nhtvvnn"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # Create new threads\n",
    "    thread = Publisher(LAP,LAR,LHP,LHR,LKP,RAP,RAR,RHP,\\\n",
    "        RHR,RKP,rate)\n",
    "\n",
    "    # Start new Threads\n",
    "    thread.start()\n",
    "    listener()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oo9fuaJ2vvnn",
    "outputId": "918f3f8d-1e3b-45d4-bed0-93dcdba8853e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listener\n",
      "Episode: 1, total numsteps: 33, episode steps: 33, reward: -79.42 \n",
      "Episode: 2, total numsteps: 63, episode steps: 30, reward: -99.09 \n",
      "Episode: 3, total numsteps: 93, episode steps: 30, reward: -98.28 \n",
      "Episode: 4, total numsteps: 121, episode steps: 28, reward: -101.39 \n",
      "Episode: 5, total numsteps: 157, episode steps: 36, reward: -100.28 \n",
      "Episode: 6, total numsteps: 190, episode steps: 33, reward: -77.72 \n",
      "Episode: 7, total numsteps: 223, episode steps: 33, reward: -103.13 \n",
      "Episode: 8, total numsteps: 259, episode steps: 36, reward: -102.2 \n",
      "Episode: 9, total numsteps: 277, episode steps: 18, reward: -85.67 \n",
      "Episode: 10, total numsteps: 288, episode steps: 11, reward: -94.43 \n",
      "Episode: 11, total numsteps: 301, episode steps: 13, reward: -98.46 \n",
      "Episode: 12, total numsteps: 320, episode steps: 19, reward: -101.12 \n",
      "Episode: 13, total numsteps: 343, episode steps: 23, reward: -101.67 \n",
      "Episode: 14, total numsteps: 358, episode steps: 15, reward: -101.06 \n",
      "Episode: 15, total numsteps: 377, episode steps: 19, reward: -100.09 \n",
      "Episode: 16, total numsteps: 396, episode steps: 19, reward: -101.85 \n",
      "Episode: 17, total numsteps: 416, episode steps: 20, reward: -95.6 \n",
      "Episode: 18, total numsteps: 445, episode steps: 29, reward: -93.56 \n",
      "Episode: 19, total numsteps: 462, episode steps: 17, reward: -101.7 \n",
      "Episode: 20, total numsteps: 485, episode steps: 23, reward: -99.4 \n",
      "Episode: 21, total numsteps: 500, episode steps: 15, reward: -100.1 \n",
      "Episode: 22, total numsteps: 514, episode steps: 14, reward: -87.35 \n",
      "Episode: 23, total numsteps: 533, episode steps: 19, reward: -101.88 \n",
      "Episode: 24, total numsteps: 555, episode steps: 22, reward: -101.81 \n",
      "Episode: 25, total numsteps: 575, episode steps: 20, reward: -87.75 \n",
      "Episode: 26, total numsteps: 605, episode steps: 30, reward: -90.05 \n",
      "Episode: 27, total numsteps: 624, episode steps: 19, reward: -99.47 \n",
      "Episode: 28, total numsteps: 646, episode steps: 22, reward: -94.66 \n",
      "Episode: 29, total numsteps: 662, episode steps: 16, reward: -100.68 \n",
      "Episode: 30, total numsteps: 675, episode steps: 13, reward: -97.57 \n",
      "Episode: 31, total numsteps: 690, episode steps: 15, reward: -95.95 \n",
      "Episode: 32, total numsteps: 713, episode steps: 23, reward: -92.46 \n",
      "Episode: 33, total numsteps: 730, episode steps: 17, reward: -100.81 \n",
      "Episode: 34, total numsteps: 742, episode steps: 12, reward: -99.34 \n",
      "Episode: 35, total numsteps: 762, episode steps: 20, reward: -91.02 \n",
      "Episode: 36, total numsteps: 788, episode steps: 26, reward: -101.56 \n",
      "Episode: 37, total numsteps: 806, episode steps: 18, reward: -92.8 \n",
      "Episode: 38, total numsteps: 830, episode steps: 24, reward: -95.98 \n",
      "Episode: 39, total numsteps: 842, episode steps: 12, reward: -98.54 \n",
      "Episode: 40, total numsteps: 867, episode steps: 25, reward: -54.94 \n",
      "Episode: 41, total numsteps: 884, episode steps: 17, reward: -90.73 \n",
      "Episode: 42, total numsteps: 911, episode steps: 27, reward: -86.09 \n",
      "Episode: 43, total numsteps: 947, episode steps: 36, reward: -87.69 \n",
      "Episode: 44, total numsteps: 961, episode steps: 14, reward: -101.4 \n",
      "Episode: 45, total numsteps: 980, episode steps: 19, reward: -90.92 \n",
      "Episode: 46, total numsteps: 996, episode steps: 16, reward: -95.04 \n",
      "Episode: 47, total numsteps: 1009, episode steps: 13, reward: -91.86 \n",
      "Episode: 48, total numsteps: 1023, episode steps: 14, reward: -100.05 \n",
      "Episode: 49, total numsteps: 1042, episode steps: 19, reward: -92.5 \n",
      "Episode: 50, total numsteps: 1060, episode steps: 18, reward: -88.6 \n",
      "Episode: 51, total numsteps: 1080, episode steps: 20, reward: -95.57 \n",
      "Episode: 52, total numsteps: 1097, episode steps: 17, reward: -99.32 \n",
      "Episode: 53, total numsteps: 1121, episode steps: 24, reward: -71.62 \n",
      "Episode: 54, total numsteps: 1141, episode steps: 20, reward: -96.41 \n",
      "Episode: 55, total numsteps: 1156, episode steps: 15, reward: -100.6 \n",
      "Episode: 56, total numsteps: 1174, episode steps: 18, reward: -98.11 \n",
      "Episode: 57, total numsteps: 1192, episode steps: 18, reward: -94.48 \n",
      "Episode: 58, total numsteps: 1203, episode steps: 11, reward: -101.09 \n",
      "Episode: 59, total numsteps: 1217, episode steps: 14, reward: -100.93 \n",
      "Episode: 60, total numsteps: 1233, episode steps: 16, reward: -86.9 \n",
      "Episode: 61, total numsteps: 1244, episode steps: 11, reward: -100.57 \n",
      "Episode: 62, total numsteps: 1256, episode steps: 12, reward: -99.13 \n",
      "Episode: 63, total numsteps: 1281, episode steps: 25, reward: -75.67 \n",
      "Episode: 64, total numsteps: 1294, episode steps: 13, reward: -91.07 \n",
      "Episode: 65, total numsteps: 1307, episode steps: 13, reward: -98.78 \n",
      "Episode: 66, total numsteps: 1322, episode steps: 15, reward: -95.58 \n",
      "Episode: 67, total numsteps: 1336, episode steps: 14, reward: -95.18 \n",
      "Episode: 68, total numsteps: 1354, episode steps: 18, reward: -97.79 \n",
      "Episode: 69, total numsteps: 1370, episode steps: 16, reward: -101.12 \n",
      "Episode: 70, total numsteps: 1373, episode steps: 3, reward: -84.63 \n",
      "Episode: 71, total numsteps: 1393, episode steps: 20, reward: -93.48 \n",
      "Episode: 72, total numsteps: 1407, episode steps: 14, reward: -100.32 \n",
      "Episode: 73, total numsteps: 1427, episode steps: 20, reward: -96.54 \n",
      "Episode: 74, total numsteps: 1442, episode steps: 15, reward: -101.49 \n",
      "Episode: 75, total numsteps: 1468, episode steps: 26, reward: -89.42 \n",
      "Episode: 76, total numsteps: 1488, episode steps: 20, reward: -96.6 \n",
      "Episode: 77, total numsteps: 1522, episode steps: 34, reward: -60.48 \n",
      "Episode: 78, total numsteps: 1545, episode steps: 23, reward: -86.54 \n",
      "Episode: 79, total numsteps: 1566, episode steps: 21, reward: -80.25 \n",
      "Episode: 80, total numsteps: 1590, episode steps: 24, reward: -97.67 \n",
      "Episode: 81, total numsteps: 1609, episode steps: 19, reward: -101.36 \n",
      "Episode: 82, total numsteps: 1628, episode steps: 19, reward: -100.44 \n",
      "Episode: 83, total numsteps: 1648, episode steps: 20, reward: -94.32 \n",
      "Episode: 84, total numsteps: 1663, episode steps: 15, reward: -100.76 \n",
      "Episode: 85, total numsteps: 1679, episode steps: 16, reward: -95.37 \n",
      "Episode: 86, total numsteps: 1696, episode steps: 17, reward: -98.76 \n",
      "Episode: 87, total numsteps: 1716, episode steps: 20, reward: -95.31 \n",
      "Episode: 88, total numsteps: 1733, episode steps: 17, reward: -90.19 \n",
      "Episode: 89, total numsteps: 1751, episode steps: 18, reward: -101.8 \n",
      "Episode: 90, total numsteps: 1765, episode steps: 14, reward: -101.03 \n",
      "Episode: 91, total numsteps: 1780, episode steps: 15, reward: -100.64 \n",
      "Episode: 92, total numsteps: 1795, episode steps: 15, reward: -97.4 \n",
      "Episode: 93, total numsteps: 1812, episode steps: 17, reward: -97.87 \n",
      "Episode: 94, total numsteps: 1831, episode steps: 19, reward: -100.31 \n",
      "Episode: 95, total numsteps: 1847, episode steps: 16, reward: -98.04 \n",
      "Episode: 96, total numsteps: 1864, episode steps: 17, reward: -101.34 \n",
      "Episode: 97, total numsteps: 1886, episode steps: 22, reward: -77.8 \n",
      "Episode: 98, total numsteps: 1902, episode steps: 16, reward: -100.76 \n",
      "Episode: 99, total numsteps: 1920, episode steps: 18, reward: -92.48 \n",
      "Episode: 100, total numsteps: 1933, episode steps: 13, reward: -94.23 \n",
      "avereage of reward : -94.73269999999997\n",
      "Episode: 101, total numsteps: 1945, episode steps: 12, reward: -95.18 \n",
      "Episode: 102, total numsteps: 1958, episode steps: 13, reward: -82.04 \n",
      "Episode: 103, total numsteps: 1970, episode steps: 12, reward: -101.2 \n",
      "Episode: 104, total numsteps: 1991, episode steps: 21, reward: -102.1 \n",
      "Episode: 105, total numsteps: 2010, episode steps: 19, reward: -86.3 \n",
      "Episode: 106, total numsteps: 2021, episode steps: 11, reward: -99.04 \n",
      "Episode: 107, total numsteps: 2038, episode steps: 17, reward: -98.19 \n",
      "Episode: 108, total numsteps: 2052, episode steps: 14, reward: -88.81 \n",
      "Episode: 109, total numsteps: 2073, episode steps: 21, reward: -101.47 \n",
      "Episode: 110, total numsteps: 2090, episode steps: 17, reward: -101.7 \n",
      "Episode: 111, total numsteps: 2106, episode steps: 16, reward: -100.71 \n",
      "Episode: 112, total numsteps: 2120, episode steps: 14, reward: -101.4 \n",
      "Episode: 113, total numsteps: 2136, episode steps: 16, reward: -100.63 \n",
      "Episode: 114, total numsteps: 2151, episode steps: 15, reward: -99.68 \n",
      "Episode: 115, total numsteps: 2174, episode steps: 23, reward: -84.86 \n",
      "Episode: 116, total numsteps: 2191, episode steps: 17, reward: -98.15 \n",
      "Episode: 117, total numsteps: 2214, episode steps: 23, reward: -35.92 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 118, total numsteps: 2228, episode steps: 14, reward: -101.38 \n",
      "Episode: 119, total numsteps: 2241, episode steps: 13, reward: -101.3 \n",
      "Episode: 120, total numsteps: 2256, episode steps: 15, reward: -86.38 \n",
      "Episode: 121, total numsteps: 2284, episode steps: 28, reward: -100.29 \n",
      "Episode: 122, total numsteps: 2304, episode steps: 20, reward: -91.28 \n",
      "-----------tool_keshid_oomadam_biroon--------\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok1\n",
      "ok2\n",
      "ok2\n",
      "ok2\n",
      "ok2\n",
      "ok2\n",
      "ok2\n",
      "Saving buffer to checkpoints/sac_buffer_biped_3_original\n",
      "ok2\n",
      "ok2\n",
      "ok2\n"
     ]
    }
   ],
   "source": [
    "agent.critic.save_weights(\"original_weight_critic_3\")\n",
    "print(\"ok1\")\n",
    "agent.critic_target.save_weights(\"original_weight_target_critic_3\")\n",
    "print(\"ok2\")\n",
    "np.save(\"target_entropy_3.npy\",np.array(agent.target_entropy))\n",
    "print(\"ok2\")\n",
    "\n",
    "np.save(\"log_alpha_3.npy\",np.array(agent.log_alpha))\n",
    "print(\"ok2\")\n",
    "\n",
    "agent.policy.save_weights(\"original_weight_policy_3\")\n",
    "print(\"ok2\")\n",
    "\n",
    "\n",
    "np.save(\"updates_3.npy\",np.array(updates))\n",
    "print(\"ok2\")\n",
    "\n",
    "\n",
    "\n",
    "np.save(\"total_numsteps_3.npy\",np.array(total_numsteps))\n",
    "print(\"ok2\")\n",
    "\n",
    "memory.save_buffer(\"biped_3\", \"original\")\n",
    "print(\"ok2\")\n",
    "\n",
    "np.save(\"i_episode_3.npy\",np.array(i_episode))\n",
    "print(\"ok2\")\n",
    "\n",
    "\n",
    "np.save(\"alpha_3.npy\",np.array(agent.alpha))\n",
    "print(\"ok2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading buffer from checkpoints/sac_buffer_biped_3_original\n"
     ]
    }
   ],
   "source": [
    "agent.critic.load_weights(\"original_weight_critic_3\")\n",
    "agent.critic_target.load_weights(\"original_weight_target_critic_3\")\n",
    "agent.target_entropy = tf.constant(np.load(\"target_entropy_3.npy\"),dtype=tf.float64)\n",
    "agent.log_alpha = tf.Variable(np.load(\"log_alpha_3.npy\"),dtype=tf.float64)\n",
    "agent.policy.load_weights(\"original_weight_policy_3\")\n",
    "updates =int(np.load(\"updates_3.npy\"))\n",
    "total_numsteps =int(np.load(\"total_numsteps_3.npy\"))\n",
    "memory.load_buffer(\"checkpoints/sac_buffer_biped_3_original\")\n",
    "i_episode =int(np.load(\"i_episode_3.npy\"))\n",
    "agent.alpha =np.load(\"alpha_3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading buffer from checkpoints/sac_buffer_biped_original\n"
     ]
    }
   ],
   "source": [
    "agent.critic.load_weights(\"original_weight_critic_2\")\n",
    "agent.critic_target.load_weights(\"original_weight_target_critic_2\")\n",
    "agent.target_entropy = tf.constant(np.load(\"target_entropy.npy\"),dtype=tf.float64)\n",
    "agent.log_alpha = tf.Variable(np.load(\"log_alpha.npy\"),dtype=tf.float64)\n",
    "agent.policy.load_weights(\"original_weight_policy_2\")\n",
    "updates =int(np.load(\"updates.npy\"))\n",
    "total_numsteps =int(np.load(\"total_numsteps.npy\"))\n",
    "memory.load_buffer(\"checkpoints/sac_buffer_biped_original\")\n",
    "i_episode =int(np.load(\"i_episode.npy\"))\n",
    "agent.alpha =np.load(\"alpha.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djgHNP9yKSUH"
   },
   "outputs": [],
   "source": [
    "# Repeat until convergence\n",
    "\n",
    "def publisher(LAP   ,LHP      ,LKP,RAP   ,RHP   ,   RKP,rate,counter):\n",
    "\n",
    "            count = 0\n",
    "        \n",
    "            render = False\n",
    "            start_steps=0 \n",
    "            verbose=False\n",
    "            epochs=50\n",
    "            batch_size=256\n",
    "\n",
    "            global_step = 1\n",
    "            global episode\n",
    "            global episode_rewards\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            while not rospy.is_shutdown() :\n",
    "               \n",
    "                #if count == 1:\n",
    "                #    break\n",
    "\n",
    "                # Observe state\n",
    "                if break_loop == True:\n",
    "                    print(episode_rewards)\n",
    "                    break\n",
    "\n",
    "                    \n",
    "                best_reset()\n",
    "\n",
    "                current_state =robot_state.robot_state\n",
    "\n",
    "                step = 1\n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "                robot_state.done=False\n",
    "                while not done:\n",
    "\n",
    "\n",
    "                    if global_step < start_steps:\n",
    "\n",
    "                        if np.random.uniform() > 0.8:\n",
    "                            action = uniform_sampling.sample()\n",
    "                        else:\n",
    "                            action = sac.sample_action(current_state)\n",
    "                    else:\n",
    "                        action = sac.sample_action(current_state)\n",
    "\n",
    "                    # Execute action, observe next state and reward\n",
    "                    #print(\"action: \",action,\"****\",robot_state.robot_state)\n",
    "                    #count = count  + 1 \n",
    "                    #print(\"action : \" , action)\n",
    "                    #print(\"***************\")\n",
    "                    reward,done = take_action(action)#ros\n",
    "                    #print(reward,done)\n",
    "                    #print(\"****************************************\")\n",
    "                    next_state = robot_state.robot_state\n",
    "\n",
    "                    episode_reward +=  reward\n",
    "\n",
    "                    # Set end to 0 if the episode ends otherwise make it 1\n",
    "                    # although the meaning is opposite but it is just easier to mutiply\n",
    "                    # with reward for the last step.\n",
    "                    if done:\n",
    "                        end = 0\n",
    "                    else:\n",
    "                        end = 1\n",
    "\n",
    "                    if verbose:\n",
    "                        logging.info(f'Global step: {global_step}')\n",
    "                        logging.info(f'current_state: {current_state}')\n",
    "                        logging.info(f'action: {action}')\n",
    "                        logging.info(f'reward: {reward}')\n",
    "                        logging.info(f'next_state: {next_state}')\n",
    "                        logging.info(f'end: {end}')\n",
    "\n",
    "                    # Store transition in replay buffer\n",
    "                    \n",
    "                    replay.store(current_state, action, reward, next_state, end)\n",
    "                    # Update current state\n",
    "                    current_state = next_state\n",
    "\n",
    "                    step += 1\n",
    "                    global_step += 1\n",
    "\n",
    "\n",
    "                \n",
    "                if (step % 1 == 0) and (global_step > start_steps):\n",
    "                    for epoch in range(epochs):\n",
    "\n",
    "                        # Randomly sample minibatch of transitions from replay buffer\n",
    "                        #print(\"in fetch smaple  ............\")\n",
    "                        current_states, actions, rewards, next_states, ends = replay.fetch_sample(num_samples=batch_size)\n",
    "\n",
    "                        # Perform single step of gradient descent on Q and policy\n",
    "                        # network\n",
    "                        #print(\"in training ...............\")\n",
    "                        critic1_loss, critic2_loss, actor_loss, alpha_loss = sac.train(current_states, actions, rewards, next_states, ends)\n",
    "                        #print(\"exit.....\")\n",
    "                        if verbose:\n",
    "                            print(episode, global_step, epoch, critic1_loss.numpy(),\n",
    "                                  critic2_loss.numpy(), actor_loss.numpy(), episode_reward)\n",
    "\n",
    "\n",
    "                        with writer.as_default():\n",
    "                            tf.summary.scalar(\"actor_loss\", actor_loss, sac.epoch_step)\n",
    "                            tf.summary.scalar(\"critic1_loss\", critic1_loss, sac.epoch_step)\n",
    "                            tf.summary.scalar(\"critic2_loss\", critic2_loss, sac.epoch_step)\n",
    "                            tf.summary.scalar(\"alpha_loss\", alpha_loss, sac.epoch_step)\n",
    "\n",
    "                        sac.epoch_step += 1\n",
    "\n",
    "                        if sac.epoch_step % 1 == 0:\n",
    "                            sac.update_weights()\n",
    "\n",
    "\n",
    "                if episode % 1 == 0:\n",
    "                    sac.policy.save_weights('../data/models/' + f'{str(datetime.utcnow().date())}-{str(datetime.utcnow().time())}' + '/model')\n",
    "\n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode += 1\n",
    "\n",
    "                avg_episode_reward = sum(episode_rewards[-100:])/len(episode_rewards[-100:])\n",
    "                episode_rewards_average.append(avg_episode_reward)\n",
    "                print(f\"Episode {episode} reward: {episode_reward}\")\n",
    "                print(f\"{episode} Average episode reward: {avg_episode_reward}\")\n",
    "                with writer.as_default():\n",
    "                    tf.summary.scalar(\"episode_reward\", episode_reward, episode)\n",
    "                    tf.summary.scalar(\"avg_episode_reward\", avg_episode_reward, episode)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac.q1.save_weights(\"Q1\")\n",
    "sac.q2.save_weights(\"Q2\")\n",
    "sac.target_q1.save_weights(\"target_q1\")\n",
    "sac.target_q2.save_weights(\"target_q2\")\n",
    "print(sac.epoch_step)\n",
    "print(sac.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2-P_nmDDxUr"
   },
   "source": [
    "#منبع\n",
    "https://github.com/shakti365/soft-actor-critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac.policy.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccc8473220>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 3038, total numsteps: 52642, episode steps: 19, reward: -78.69 \n",
      "Episode: 3039, total numsteps: 52655, episode steps: 13, reward: -101.3 \n",
      "Episode: 3040, total numsteps: 52671, episode steps: 16, reward: -69.5 \n",
      "Episode: 3041, total numsteps: 52701, episode steps: 30, reward: -75.8 \n",
      "Episode: 3042, total numsteps: 52725, episode steps: 24, reward: -67.41 \n",
      "Episode: 3043, total numsteps: 52738, episode steps: 13, reward: -100.86 \n",
      "Episode: 3044, total numsteps: 52758, episode steps: 20, reward: -89.84 \n",
      "Episode: 3045, total numsteps: 52771, episode steps: 13, reward: -101.3 \n",
      "Episode: 3046, total numsteps: 52789, episode steps: 18, reward: -101.8 \n",
      "Episode: 3047, total numsteps: 52802, episode steps: 13, reward: -99.97 \n",
      "Episode: 3048, total numsteps: 52815, episode steps: 13, reward: -100.97 \n",
      "Episode: 3049, total numsteps: 52839, episode steps: 24, reward: -97.16 \n",
      "Episode: 3050, total numsteps: 52852, episode steps: 13, reward: -100.95 \n",
      "Episode: 3051, total numsteps: 52858, episode steps: 6, reward: -100.28 \n",
      "Episode: 3052, total numsteps: 52877, episode steps: 19, reward: -94.08 \n",
      "Episode: 3053, total numsteps: 52891, episode steps: 14, reward: -80.72 \n",
      "Episode: 3054, total numsteps: 52896, episode steps: 5, reward: -100.45 \n",
      "Episode: 3055, total numsteps: 52918, episode steps: 22, reward: -63.23 \n",
      "Episode: 3056, total numsteps: 52939, episode steps: 21, reward: -88.08 \n"
     ]
    }
   ],
   "source": [
    "plt.plot(range(len(av_rewards)),av_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-19.8196, -16.537799999999994]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SAC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
